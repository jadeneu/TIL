## 구현하게 된 과정
의약품 정보를 크롤링해야 했다. 처음엔 `BeautifulSoup`으로만 구현했지만, 페이지가 동적 사이트였기 때문에 copy selector 값이 나오지 않았고 제대로 코드가 동작하지 않았다. 그래서 `Selenium`으로 크롤러를 만들게 되었다.
<br><br>


## 구현한 방법
### 1. 사전 작업
**(1) 먼저 db와 연결을 시킨다.**

<img src="https://user-images.githubusercontent.com/55045377/127597436-7abe7839-6896-4b60-865e-b51f81b47ace.png" width=70% height=70%>

**(2) geckodriver 경로를 지정해주고 특정 url을 로드한다.**

<img src="https://user-images.githubusercontent.com/55045377/127817234-f7dda521-be49-49d4-9522-dfbbe9614bd9.png" width=80% height=80%>

* `driver`가 `webdriver`의 객체이며, 나는 파이어폭스를 사용하여 크롤링을 했다.
* `implicitly_wait`을 통해 암묵적으로 모든 자원이 로드될 때까지 기다리게 하는 시간을 지정해주었다.
* db와 연결시키기 위해 db 정보를 명시해준다.

<br><br>

### 2. search() 함수
search는 크롤링을 하는 도중 끊기게 되었을 때, 받아온 데이터를 중복으로 받아오는 것을 방지하기 위해 구현하게 되었다.<br>
```python
# dbconfig.py
def search(self, table_name, col):
    sql = "SELECT {0} FROM {1}".format(','.join(col), table_name)
    self.curs.execute(sql)
    rows = self.curs.fetchall()
        
    return rows
```
```python
# maindb.py
company_name = driver.find_element_by_xpath('/html/body/div[2]/form/div[1]/main/div[3]/article/table/tbody/tr[1]/td')
supplement_name = driver.find_element_by_xpath('/html/body/div[2]/form/div[1]/main/div[3]/article/table/tbody/tr[2]/td')
                    
# supplements 테이블 데이터 가져오기
rows = mysql_controller.search('supplements', ['supplement_name','company_name'])

check = (supplement_name.text, company_name.text)
print(check)
if check not in rows:
    print('통과')
    ...
    mysql_controller.insert('supplements', [...], ...)
```
위와 같이 가져오려는 정보인 `company_name`,`supplement_name`이 supplements 테이블에 없다면 중복 제품이 아니므로 `insert()`를 진행한다.
<br><br>

### 3. insert() 함수
`insert()` 함수를 통해 크롤링한 데이터를 db에 insert한다.<br>
```python
# dbconfig.py
def insert(self,table_name, col, values):
        sql = "INSERT INTO {0} ({1}) VALUES ({2})".format(table_name, ','.join(col), ','.join(values))
        self.curs.execute(sql)
        self.conn.commit()
```

insert를 하기 위해서는 `INSERT INTO table_name (col) VALUES ("value")` 와 같은 형태가 나와야 했다.<br>
그런데 `col`과 `values`를 인자로 넘겨서 join으로 문자열 변환을 하면 큰 따옴표는 사라지고 문자열 그대로 값이 넘겨지는 문제가 있었다.<br>
그래서 큰따옴표를 포함하여 `values`를 보내야 했고, 다음과 같은 형태의 리스트가 인자로 넘겨져야 했다.
```
['"비타민"','"(주)비타민"']
```
위 처럼 따옴표 안에 따옴표가 있어야 join을 통해 문자열 변환이 되어도 따옴표가 포함된 문자열이 value 값이 될 수 있었다.

예를 들어보자면,

insert를 아래와 같은 형식으로 했을 때 `detail_sup_list+[f'"{ingredients_str}"']`는 `values`에 해당하는 값이며, `['"비타민"','"(주)비타민"']`와 같은 형식을 띤다.

<img src="https://user-images.githubusercontent.com/55045377/127600800-978df8d8-6d12-420c-9ec3-8cfd22438cab.png" width=80% height=80%>

* **`detail_sup_list` 가 만들어진 로직**
  ```python
  supplement_detail_xpath = '/html/body/div[2]/form/div[1]/main/div[3]/article/table/tbody/tr[{}]/td'
  detail_sup_list = []
  for i in range(1,13):
      supplement_detail = driver.find_element_by_xpath(supplement_detail_xpath.format(i)).text
      detail_sup_list.append(f'"{supplement_detail}"')
  ```
  * `supplement_detail_xpath`는 제품 정보가 나열되어 있는 페이지에서 전체 제품 정보를 포함할 수 있는 xpath이다.
  * 한 페이지 당 12가지의 정보가 있기 때문에 for문의 range는 1부터 13까지 반복한다.
  * `supplement_detail`에 각 의약품 정보를 담고, `detail_sup_list`에 `f'"{supplement_detail}"'`형태로 삽입한다.

* **`ingredients_str` 가 만들어진 로직**
  ```python
  ingredients_xpath = '/html/body/div[2]/form/div[1]/main/div[3]/article/div[{}]/table/tbody/tr'
  ingredients_str = ""
  for i in range(1,4):
      get_ingres = driver.find_elements_by_xpath(ingredients_xpath.format(i))
      for ingre in get_ingres:
          if ingre.text == "원재료 정보가 없습니다.":
              break
          else:
              ingredients_str += ''.join(ingre.text.split()[1:])
              ingredients_str += ','
  ```
  * `ingredients_xpath`는 의약품 정보 페이지에서 '재료'에 해당하는 전체 틀을 포함하는 xpath이다.
  * 재료 정보는 기능성 원재료 정보, 기타 원재료 정보, 캡슐 원재료 정보로 항상 3가지 정보가 명시되어있기 때문에 for문의 range는 1부터 4까지 반복한다.
  * `get_ingres`로 재료 정보들을 가져오고, for문을 통해 각각의 정보에 따라 `break`로 for문을 빠져나가거나 `ingredients_str`에 정보값을 더한다.
  * `ingredients_str`의 형태는 '청국장균배양정제물,사과향혼합제제,사과농축액,액상과당,...' 이며, `[f'"{ingredients_str}"']`를 통해 만들고자 한 형태가 나오게 된다.

<br><br>

---

셀레니움을 이용한 크롤링을 하면서 알 수 없는 오류에 답답하고 막막하기도 했지만, 크롤링을 잘 마쳐서 다행이다.<br>
이 데이터를 json 형태로 받기 위해 위의 크롤링 형태에서 수정을 거쳐 최종적으로는 다른 형태로 크롤링을 진행했지만, 셀레니움 크롤링에 대해 확실하게 이해하고 경험한 것에 만족한다.<br>
다른 프로젝트에서 다시 크롤링을 해야한다면 보다 수월하게 크롤링을 진행할 수 있을 거라 생각한다.
<br><br><br>























